{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress sklearn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv,find_dotenv\n",
    "# import os\n",
    "# # use the find_dotenv function to locate the file\n",
    "# load_dotenv(find_dotenv())\n",
    "\n",
    "# db_string = os.getenv(\"DB_CONN\") or \"postgresql://postgres:password@localhost:5432/rec_llm\"\n",
    "# # define a psycopg3 connection to postgres\n",
    "# conn = psycopg.connect(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv_stack_post = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=8, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.conv_stack_prompt = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=8, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(64, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(1027, 1024, bias=False),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 1024, bias=False),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512, bias=False),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 10, bias=False),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # take the first 2048 values for the embeddings\n",
    "        prompt_embedding = x[:, :1024]\n",
    "        post_embedding = x[:, 1024:2048]\n",
    "        features = x[:, 2048:]\n",
    "        # take the rest of the values for the features\n",
    "        prompt_embedding = prompt_embedding.unsqueeze(1)\n",
    "        post_embedding = post_embedding.unsqueeze(1)\n",
    "\n",
    "        prompt_embedding = self.conv_stack_prompt(prompt_embedding)\n",
    "        post_embedding = self.conv_stack_post(post_embedding)\n",
    "\n",
    "        prompt_embedding = self.flatten(prompt_embedding)\n",
    "        post_embedding = self.flatten(post_embedding)\n",
    "\n",
    "        # embeddings = self.flatten(embeddings)\n",
    "        x = torch.cat([prompt_embedding, post_embedding, features], dim=1)\n",
    "        # print(x.shape)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "def get_model():\n",
    "    model = NeuralNetwork().to(device)\n",
    "    model.apply(init_weights)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('intfloat/e5-large-v2')\n",
    "\n",
    "CATEGORIES = [\n",
    "    'autos',\n",
    " 'entertainment',\n",
    " 'finance',\n",
    " 'foodanddrink',\n",
    " 'health',\n",
    " 'kids',\n",
    " 'lifestyle',\n",
    " 'middleeast',\n",
    " 'movies',\n",
    " 'music',\n",
    " 'news',\n",
    " 'northamerica',\n",
    " 'sports',\n",
    " 'travel',\n",
    " 'tv',\n",
    " 'video',\n",
    " 'weather']\n",
    "CATEGORIES.sort()\n",
    "\n",
    "def load_model(path):\n",
    "    model =  get_model()\n",
    "    if device == 'cpu':\n",
    "        model.load_state_dict(torch.load(path, map_location='cpu'))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def infer(processed_tensor: torch.Tensor) -> float | list[float]:\n",
    "    with torch.no_grad():\n",
    "        logits = MODEL(processed_tensor)\n",
    "        # recursively flatten logits and return an float or list of floats\n",
    "        logits = logits.cpu().numpy().flatten().tolist()\n",
    "        return logits\n",
    "\n",
    "REQUIRED_FIELDS = [\n",
    "    # 'title',\n",
    "    # 'abstract',\n",
    "    'prompt',\n",
    "    'prompt_type',\n",
    "    'post_category',\n",
    "    'prompt_category'\n",
    "]\n",
    "\n",
    "# @cache\n",
    "# def get_embedding(text: str | list[str]):\n",
    "#     return embedding_model.encode(text)\n",
    "\n",
    "def process_input(**kwargs):\n",
    "    for field in REQUIRED_FIELDS:\n",
    "        if field not in kwargs:\n",
    "            raise ValueError(f\"Missing required field: {field}\")\n",
    "\n",
    "    # try:\n",
    "    #     post = kwargs['title'] + kwargs['abstract']\n",
    "    # except:\n",
    "    #     post = kwargs['title']\n",
    "    prompt = kwargs['prompt_embedding']\n",
    "    post = kwargs['post_embedding']\n",
    "\n",
    "    embedded_prompt = None\n",
    "    embedded_post = None\n",
    "    # if kwargs['prompt_embedding'] == 'False':\n",
    "    #     embedded_prompt = get_embedding(prompt)\n",
    "\n",
    "    # if kwargs['post_embedding'] == 'False':\n",
    "    #     embedded_post = get_embedding(post)\n",
    "\n",
    "    prompt_type = kwargs['prompt_type']\n",
    "\n",
    "    prompt_type_map = {\n",
    "        'boost': 0,\n",
    "        'suppress': 1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        prompt_type = prompt_type_map[prompt_type]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Invalid prompt type: {prompt_type}\")\n",
    "\n",
    "    post_category = kwargs['post_category']\n",
    "    prompt_category = kwargs['prompt_category']\n",
    "\n",
    "    # check if post_category and prompt_category are in the list of categories\n",
    "    if post_category not in CATEGORIES:\n",
    "        raise ValueError(f\"Invalid post category: {post_category}\")\n",
    "\n",
    "    if prompt_category not in CATEGORIES:\n",
    "        raise ValueError(f\"Invalid prompt category: {prompt_category}\")\n",
    "\n",
    "    # make a one hot encoding of the categories\n",
    "    post_category_index = CATEGORIES.index(post_category)\n",
    "    prompt_category_index = CATEGORIES.index(prompt_category)\n",
    "\n",
    "    post_category = [0] * len(CATEGORIES)\n",
    "    post_category[post_category_index] = 1\n",
    "\n",
    "    prompt_category = [0] * len(CATEGORIES)\n",
    "    prompt_category[prompt_category_index] = 1\n",
    "\n",
    "    # try:\n",
    "    #     prompt = np.array(eval(prompt))\n",
    "    # except:\n",
    "    #     print(prompt)\n",
    "    #     raise ValueError(f\"Invalid prompt: {prompt}\")\n",
    "\n",
    "    # try:\n",
    "    #     post = np.array(eval(post))\n",
    "    # except:\n",
    "    #     print(post)\n",
    "    #     raise ValueError(f\"Invalid post: {post}\")\n",
    "    features = np.concatenate([\n",
    "        prompt,\n",
    "        post,\n",
    "        [prompt_type],\n",
    "        post_category,\n",
    "        prompt_category\n",
    "    ])\n",
    "\n",
    "\n",
    "    # print(features.shape)\n",
    "    # print(features)\n",
    "    return features\n",
    "\n",
    "# def get_embedding(text: str | list[str]):\n",
    "#     return embedding_model.encode(text, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (conv_stack_post): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(8,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv1d(64, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv_stack_prompt): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(8,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv1d(64, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=1027, out_features=1024, bias=False)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Linear(in_features=512, out_features=256, bias=False)\n",
       "    (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): Dropout(p=0.2, inplace=False)\n",
       "    (16): Linear(in_features=256, out_features=256, bias=False)\n",
       "    (17): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Dropout(p=0.2, inplace=False)\n",
       "    (20): Linear(in_features=256, out_features=256, bias=False)\n",
       "    (21): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU()\n",
       "    (23): Dropout(p=0.2, inplace=False)\n",
       "    (24): Linear(in_features=256, out_features=10, bias=False)\n",
       "    (25): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU()\n",
       "    (27): Linear(in_features=10, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = load_model('./api/model.bin')\n",
    "MODEL.to(device)\n",
    "MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(row):\n",
    "    features = process_input(**row)\n",
    "    # print(features)\n",
    "    features = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    return infer(features)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(true_ranks, predicted_ranks):\n",
    "    \"\"\"\n",
    "    Calculates the mean reciprocal rank (MRR) for the given true ranks and predicted ranks.\n",
    "\n",
    "    Args:\n",
    "        true_ranks (list): A list of true ranks.\n",
    "        predicted_ranks (list): A list of lists, where each inner list represents the predicted ranks.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean reciprocal rank.\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    for true_rank, predicted_rank in zip(true_ranks, predicted_ranks):\n",
    "        # If the true rank is not in the predicted ranks, the reciprocal rank is 0\n",
    "        if true_rank not in predicted_rank:\n",
    "            reciprocal_rank = 0.0\n",
    "        else:\n",
    "            # Get the position of the true rank in the predicted ranks\n",
    "            rank_position = predicted_rank.index(true_rank) + 1\n",
    "            reciprocal_rank = 1.0 / rank_position\n",
    "\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "\n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    return mrr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_from_api():\n",
    "    url = \"https://train.synapse.com.np/training_data\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_from_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3230"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list of dicts\n",
    "\"\"\"\n",
    "eg: a dict {a: {b: 1, c: 2}, d: 3} will be converted to {a_b: 1, a_c: 2, d: 3}\n",
    "\"\"\"\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# flatten data\n",
    "data = [flatten_dict(d) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2826,\n",
       " 'post_title': 'How to Get Rid of Skin Tags, According to a Dermatologist',\n",
       " 'post_abstract': \"They seem harmless, but there's a very good reason you shouldn't ignore them. The post How to Get Rid of Skin Tags, According to a Dermatologist appeared first on Reader's Digest.\",\n",
       " 'post_category': 'health',\n",
       " 'post_subcategory': 'medical',\n",
       " 'prompt_prompt': 'Show updates on scientific breakthroughs in medicine.',\n",
       " 'prompt_type': 'boost',\n",
       " 'prompt_category': 'health',\n",
       " 'prompt_subcategory': 'medical',\n",
       " 'label': 0.8}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def embed(text: str | list[str]):\n",
    "    # check if the embeddingmodel is mixedbread-ai/mxbai-embed-large-v1\n",
    "    return embedding_model.encode(text, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3230 entries, 0 to 3229\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  3230 non-null   int64  \n",
      " 1   post_title          3230 non-null   object \n",
      " 2   post_abstract       3061 non-null   object \n",
      " 3   post_category       3230 non-null   object \n",
      " 4   post_subcategory    3230 non-null   object \n",
      " 5   prompt_prompt       3230 non-null   object \n",
      " 6   prompt_type         3230 non-null   object \n",
      " 7   prompt_category     3230 non-null   object \n",
      " 8   prompt_subcategory  3230 non-null   object \n",
      " 9   label               3230 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(8)\n",
      "memory usage: 252.5+ KB\n"
     ]
    }
   ],
   "source": [
    "t.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories():\n",
    "    api = \"https://train.synapse.com.np/categories\"\n",
    "    response = requests.get(api)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autos',\n",
       " 'entertainment',\n",
       " 'finance',\n",
       " 'foodanddrink',\n",
       " 'health',\n",
       " 'kids',\n",
       " 'lifestyle',\n",
       " 'middleeast',\n",
       " 'movies',\n",
       " 'music',\n",
       " 'news',\n",
       " 'northamerica',\n",
       " 'sports',\n",
       " 'travel',\n",
       " 'tv',\n",
       " 'video',\n",
       " 'weather']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = get_categories()\n",
    "categories.sort()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pl.DataFrame(data)\n",
    "x = x.with_columns(\n",
    "        pl.concat_str([\n",
    "            pl.col(\"post_title\"),\n",
    "            pl.col(\"post_abstract\"),\n",
    "        ]).alias(\"post\")\n",
    "    ).with_columns( # handle null values for post\n",
    "        pl.when(pl.col(\"post\").is_null())\n",
    "        .then(pl.col(\"post_title\"))\n",
    "        .otherwise(pl.col(\"post\")).alias(\"post\")\n",
    "    ).with_columns( # create embeddings for the post and prompt\n",
    "        pl.col(\"post\").map_elements(embed).alias(\"post_embedding\"),\n",
    "        pl.col(\"prompt_prompt\").map_elements(embed).alias(\"prompt_embedding\"),\n",
    "        pl.col(\"label\").cast(pl.Float32).alias(\"label\") # cast the label to a float\n",
    "    )\n",
    "\n",
    "x.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_data):\n",
    "    # df = pl.DataFrame(raw_data, schema= {'id': pl.Int32, 'title': pl.Utf8, 'abstract': pl.Utf8, 'prompt': pl.Utf8, 'prompt_type': pl.Utf8, 'label': pl.Utf8})\n",
    "    df = pl.DataFrame(raw_data)\n",
    "    # concat the title and abstract\n",
    "    df = df.with_columns(\n",
    "        pl.concat_str([\n",
    "            pl.col(\"post_title\"),\n",
    "            pl.col(\"post_abstract\"),\n",
    "        ]).alias(\"post\")\n",
    "    ).with_columns( # handle null values for post\n",
    "        pl.when(pl.col(\"post\").is_null())\n",
    "        .then(pl.col(\"post_title\"))\n",
    "        .otherwise(pl.col(\"post\")).alias(\"post\")\n",
    "    ).with_columns( # create embeddings for the post and prompt\n",
    "        pl.col(\"post\").map_elements(embed).alias(\"post_embedding\"),\n",
    "        pl.col(\"prompt_prompt\").map_elements(embed).alias(\"prompt_embedding\"),\n",
    "        pl.col(\"label\").cast(pl.Float32).alias(\"label\") # cast the label to a float\n",
    "    )\n",
    "\n",
    "    df = df.to_pandas()\n",
    "\n",
    "\n",
    "    # df['prompt_type'] = df['prompt_type'].map({'suppress': 0, 'boost': 1})\n",
    "    # df = df[['id', 'post', 'post_embedding', 'prompt', 'prompt_embedding','prompt_type', 'post_category', 'prompt_category', 'label']]\n",
    "\n",
    "    # for category in categories:\n",
    "    #     df[f'prompt_{category}'] = (df['prompt_category'] == category).astype(int)\n",
    "\n",
    "    # for category in categories:\n",
    "    #     df[f'post_{category}'] = (df['post_category'] == category).astype(int)\n",
    "\n",
    "    # df = df.drop(columns=['post_category', 'prompt_category'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'post_title', 'post_abstract', 'post_category',\n",
       "       'post_subcategory', 'prompt_prompt', 'prompt_type', 'prompt_category',\n",
       "       'prompt_subcategory', 'label', 'post', 'post_embedding',\n",
       "       'prompt_embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename df.prompt_prompt to df.prompt\n",
    "df = df.rename(columns={\n",
    "    'prompt_prompt': 'prompt',\n",
    "    'post_title': 'title',\n",
    "    'post_abstract': 'abstract'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'abstract', 'post_category', 'post_subcategory',\n",
       "       'prompt', 'prompt_type', 'prompt_category', 'prompt_subcategory',\n",
       "       'label', 'post', 'post_embedding', 'prompt_embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = df.groupby('prompt').filter(lambda x: len(x) >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3179, 13)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0.0050256783, -0.054736964, 0.009037347, -0.0...\n",
       "1       [-0.014443579, -0.057925355, 0.011125997, -0.0...\n",
       "2       [-0.011852036, -0.06560517, 0.003943302, -0.00...\n",
       "3       [-0.006226413, -0.050020713, 0.034159217, -0.0...\n",
       "4       [-0.021042276, -0.07230517, 0.021734122, -0.01...\n",
       "                              ...                        \n",
       "3225    [-0.020555772, -0.06798802, 0.022750273, 0.000...\n",
       "3226    [-0.009810819, -0.048323907, 0.016124412, -0.0...\n",
       "3227    [-0.009810819, -0.048323907, 0.016124412, -0.0...\n",
       "3228    [-0.031405423, -0.045943644, 0.007384266, 0.02...\n",
       "3229    [0.0016587655, -0.050278198, 0.006163819, -0.0...\n",
       "Name: prompt_embedding, Length: 3223, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df['prompt_embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY = \"\"\"\n",
    "#         select\n",
    "#             l.id,\n",
    "#             l.post_id,\n",
    "#                 p.title, p.abstract, p.category post_category, p_emb.embedding post_embedding,\n",
    "#             l.prompt_id,\n",
    "#                 pr.prompt, pr.prompt_type, pr.category prompt_category, pr_emb.embedding prompt_embedding,\n",
    "#             l.label from labels l\n",
    "#         join posts p on l.post_id = p.id\n",
    "#         join prompts pr on l.prompt_id = pr.id\n",
    "#         join posts_emb p_emb on l.post_id = p_emb.id\n",
    "#         join prompts_emb pr_emb on l.prompt_id = pr_emb.id\n",
    "#         order by random()\n",
    "#         ;\n",
    "#     \"\"\"\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute(QUERY)\n",
    "# labels = cursor.fetchall()\n",
    "# source_df = pd.DataFrame(labels, columns=[\"id\", \"post_id\", \"title\", \"abstract\", \"post_category\", \"post\", \"prompt_id\", \"prompt_text\", \"prompt_type\", \"prompt_category\", \"prompt\",  \"label\" ])\n",
    "\n",
    "source_df['inference'] = source_df.apply(get_inference, axis=1)\n",
    "source_df['inference'] = source_df['inference'].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(row):\n",
    "    post = row['post_embedding']\n",
    "    prompt = row['prompt_embedding']\n",
    "    # prompt = np.array(eval(prompt))\n",
    "    # post = np.array(eval(post))\n",
    "    # print(post)\n",
    "    post = torch.tensor(post, dtype=torch.float32)\n",
    "    prompt = torch.tensor(prompt, dtype=torch.float32)\n",
    "\n",
    "    cosine = F.cosine_similarity(post, prompt, dim=0)\n",
    "    if row['prompt_type'] == 'suppress':\n",
    "        return 1 - cosine.item()\n",
    "    return cosine.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df['cosine'] = source_df.apply(get_cosine, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(source_df):\n",
    "    df = source_df.groupby('prompt').apply(lambda x: x.sample(5)).reset_index(drop=True)\n",
    "    mrrs = []\n",
    "    for name, group in df.groupby('prompt'):\n",
    "        group['label_rank'] = group['label'].rank(method='dense')\n",
    "        group['inference_rank'] = group['inference'].rank(method='dense')\n",
    "        true_ranks = group['label_rank'].values\n",
    "        predicted_ranks = group['inference_rank'].values\n",
    "        mrr = mean_reciprocal_rank(true_ranks, [[x] for x in predicted_ranks])\n",
    "        mrrs.append((name, mrr))\n",
    "\n",
    "    return np.mean([mrr for name, mrr in mrrs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(relevant_items, retrieved_items):\n",
    "    \"\"\"\n",
    "    Calculates the average precision (AP) for the given relevant items and retrieved items.\n",
    "\n",
    "    Args:\n",
    "        relevant_items (list): A list of relevant items.\n",
    "        retrieved_items (list): A list of retrieved items.\n",
    "\n",
    "    Returns:\n",
    "        float: The average precision.\n",
    "    \"\"\"\n",
    "    intersections = 0\n",
    "    for i, item in enumerate(relevant_items):\n",
    "        if item in retrieved_items:\n",
    "            intersections += 1\n",
    "\n",
    "    if intersections == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return intersections / len(retrieved_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random prompt\n",
    "# for that prompt, all posts with label > 0.7 are relevant prompts\n",
    "# get the top 5 and top 10 posts for the relevant list @5 and @10\n",
    "\n",
    "# sort the posts by inference\n",
    "# get the top 5 and top 10 posts for the retrieved list @5 and @10\n",
    "\n",
    "# call the mean_average_precision on the relevant and retrieved lists @5 and @10\n",
    "\n",
    "def mean_average_precision(x, n = 5):\n",
    "    # pick a random prompt and return the prompt and corresponding posts\n",
    "    data = x.groupby('prompt').apply(lambda x: x.sample(n)).reset_index(drop=True)\n",
    "    map = []\n",
    "    for name, group in data.groupby('prompt'):\n",
    "        group['label_rank'] = group['label'].rank(method='dense')\n",
    "        group['inference_rank'] = group['inference'].rank(method='dense')\n",
    "        relevant_items = group[group['label'] > 0.65]\n",
    "\n",
    "        relevant_items_top = relevant_items.sort_values('label', ascending=False).head(n)\n",
    "        retrieved_items_top = group.sort_values('inference', ascending=False).head(n)\n",
    "\n",
    "\n",
    "        ap = average_precision(relevant_items_top['title'].values, retrieved_items_top['title'].values)\n",
    "        map.append(ap)\n",
    "    return np.mean(map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3169014084507043"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision(source_df, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34084507042253515"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision(source_df, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
